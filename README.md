# convnet_vs_single-head-attention
Trained a simple ConvNet and single-head attention model to compare for a graduate class project.

Conv2D kernel size 10 vs 3

![convnets](https://github.com/mvilsoet/convnet_vs_single-head-attention/blob/main/convnets.png)

1-head attention with/without positional encoding

![attentions](https://github.com/mvilsoet/convnet_vs_single-head-attention/blob/main/attentions.png)

Resulting attention matrix. Generally implies that the final column is enough to describe the data-space.

![attention matrix](https://github.com/mvilsoet/convnet_vs_single-head-attention/blob/main/a_matrix.png)
